{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7c_7VqT11MO",
        "outputId": "ee1b14ba-382e-45a5-bf95-c0eabb8ed37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
        "from diffusers.utils import load_image\n",
        "import json\n",
        "\n",
        "class ColorizeModel(keras.Model):\n",
        "    def __init__(self, default_prompt=None, default_negative_prompt=None, torch_model_path=None, **kwargs):\n",
        "        if 'torch_model_path' in kwargs:\n",
        "            torch_model_path = kwargs.pop('torch_model_path')\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.default_prompt = default_prompt\n",
        "        self.default_negative_prompt = default_negative_prompt\n",
        "        self.torch_model_path = torch_model_path\n",
        "\n",
        "        self.prompt_embedding = keras.layers.Dense(1, use_bias=False)\n",
        "        self.neg_prompt_embedding = keras.layers.Dense(1, use_bias=False)\n",
        "        self.conditioning_scale = keras.layers.Dense(1, use_bias=False)\n",
        "        self.guidance_scale = keras.layers.Dense(1, use_bias=False)\n",
        "        self.inference_steps = keras.layers.Dense(1, use_bias=False, dtype=tf.int32)\n",
        "\n",
        "        self.prompt_embedding.build((1, 1))\n",
        "        self.neg_prompt_embedding.build((1, 1))\n",
        "        self.conditioning_scale.build((1, 1))\n",
        "        self.guidance_scale.build((1, 1))\n",
        "        self.inference_steps.build((1, 1))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"default_prompt\": self.default_prompt,\n",
        "            \"default_negative_prompt\": self.default_negative_prompt,\n",
        "            \"torch_model_path\": self.torch_model_path\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def colorize(self, image_path, output_path=None, seed=42):\n",
        "        \"\"\"Execute image colorization using the stored PyTorch model\"\"\"\n",
        "        print(f\"Colorizing image using model from: {self.torch_model_path}\")\n",
        "        print(f\"Using prompt: {self.default_prompt}\")\n",
        "        print(f\"Using negative prompt: {self.default_negative_prompt}\")\n",
        "\n",
        "        try:\n",
        "            if isinstance(image_path, str):\n",
        "                print(f\"Loading image from path: {image_path}\")\n",
        "                image = load_image(image_path)\n",
        "            else:\n",
        "                print(\"Using provided image object\")\n",
        "                image = image_path\n",
        "\n",
        "            try:\n",
        "                from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
        "\n",
        "                print(\"Loading ControlNet model...\")\n",
        "                controlnet = ControlNetModel.from_pretrained(f\"{self.torch_model_path}/controlnet\")\n",
        "\n",
        "                print(\"Setting up StableDiffusionControlNetPipeline...\")\n",
        "                pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "                    \"runwayml/stable-diffusion-v1-5\",\n",
        "                    controlnet=controlnet,\n",
        "                    text_encoder_pretrained_model_name_or_path=f\"{self.torch_model_path}/text_encoder\",\n",
        "                    tokenizer_pretrained_model_name_or_path=f\"{self.torch_model_path}/tokenizer\",\n",
        "                    unet_pretrained_model_name_or_path=f\"{self.torch_model_path}/unet\",\n",
        "                    vae_pretrained_model_name_or_path=f\"{self.torch_model_path}/vae\"\n",
        "                )\n",
        "\n",
        "                pipeline.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                print(f\"Using device: {device}\")\n",
        "                pipeline = pipeline.to(device)\n",
        "\n",
        "                guidance_scale = 1.0  # Default value\n",
        "                cond_scale = 0.9     # Default control conditioning scale\n",
        "                steps = 25           # Default steps\n",
        "\n",
        "                generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "                print(\"Running inference...\")\n",
        "                with torch.inference_mode():\n",
        "                    output = pipeline(\n",
        "                        prompt=self.default_prompt,\n",
        "                        negative_prompt=self.default_negative_prompt,\n",
        "                        image=image,\n",
        "                        num_inference_steps=int(steps),\n",
        "                        guidance_scale=float(guidance_scale),\n",
        "                        controlnet_conditioning_scale=float(cond_scale),\n",
        "                        generator=generator\n",
        "                    )\n",
        "\n",
        "                colored_image = output.images[0]\n",
        "\n",
        "                if output_path:\n",
        "                    colored_image.save(output_path)\n",
        "                    print(f\"Image saved to {output_path}\")\n",
        "\n",
        "                return colored_image\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during PyTorch model loading or inference: {e}\")\n",
        "                print(\"This could be due to missing PyTorch components or incorrect paths\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during colorization: {e}\")\n",
        "            return None\n",
        "\n",
        "def download_and_colorize(image_path, output_path=\"colored_output.png\", username=\"YOUR_USERNAME\", seed=60):\n",
        "    repo_id = f\"{username}/image-colorizer\"\n",
        "\n",
        "    print(f\"Downloading model from {repo_id}...\")\n",
        "\n",
        "    model_file = hf_hub_download(repo_id=repo_id, filename=\"image_colorizer.h5\")\n",
        "    print(f\"Downloaded model file to: {model_file}\")\n",
        "\n",
        "    local_dir = snapshot_download(repo_id=repo_id, local_dir=\"./downloaded_model\")\n",
        "    print(f\"Downloaded model components to: {local_dir}\")\n",
        "\n",
        "    with keras.utils.custom_object_scope({'ColorizeModel': ColorizeModel}):\n",
        "        print(\"Loading the model...\")\n",
        "        loaded_model = keras.models.load_model(model_file, compile=False)\n",
        "\n",
        "        loaded_model.torch_model_path = local_dir\n",
        "\n",
        "        try:\n",
        "            with open(f\"{local_dir}/config.json\", \"r\") as f:\n",
        "                config = json.load(f)\n",
        "                if \"default_prompt\" in config:\n",
        "                    loaded_model.default_prompt = config[\"default_prompt\"]\n",
        "                if \"default_negative_prompt\" in config:\n",
        "                    loaded_model.default_negative_prompt = config[\"default_negative_prompt\"]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(\"Executing colorization...\")\n",
        "        result = loaded_model.colorize(image_path, output_path=output_path, seed=seed)\n",
        "        return result\n",
        "\n"
      ],
      "metadata": {
        "id": "5A0yCM2c1Sf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "grayscale_image = \"/content/Grayscaleimage95886.jpeg\"\n",
        "\n",
        "username = \"AmrDabour\"\n",
        "\n",
        "colorized_image = download_and_colorize(\n",
        "    grayscale_image,\n",
        "    output_path=\"colorized_result.png\",\n",
        "    username=username\n",
        ")\n",
        "\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(colorized_image)\n",
        "except:\n",
        "    print(\"Image saved to colorized_result.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "DUPQBUA521v0",
        "outputId": "f8fb73ac-24e9-4990-ec0d-f183da77b438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The config attributes {'dropout': 0.0, 'sample_size': 32} were passed to ControlNetModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
            "Keyword arguments {'text_encoder_pretrained_model_name_or_path': '/content/downloaded_model/text_encoder', 'tokenizer_pretrained_model_name_or_path': '/content/downloaded_model/tokenizer', 'unet_pretrained_model_name_or_path': '/content/downloaded_model/unet', 'vae_pretrained_model_name_or_path': '/content/downloaded_model/vae'} are not expected by StableDiffusionControlNetPipeline and will be ignored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMiusrUU3mT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}